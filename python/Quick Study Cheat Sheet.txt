[Documentation]
docs.python.org

[Current Version]
most of these notes based on 3.10 but not comprehensive with newer features
3.11 is faster and I use it on Linux, but Mac is stuck on 3.10 due to tensorflow-macos compatibility
TensorFlow currently supports 3.11 as of these notes
3.12 just came out in October 2023
    I will study it when TensorFlow switches to it

[Variables]
int, float, str, bool
    technically all reference types, but int, float, and bool are immutable
        thinking of them as primitive value types will pretty much always get you the right result though
True and False for bool
'' and "" equivalent
2 + 3j = complex
float is 64-bit minus some overhead
int is probably usually 64-bit (maybe minus some overhead) at first
    but dynamically sized for very large values so it can grow
    also some pooling/interning goes on

None = absence of value
    type is NoneType

type(x) for reflection
    type(x).__name__
dir(x) = get all the members of x
help(x) = get the docstring of x
isinstance(a, b)
NOTE: unlike in some languages, you don't need special operators to pass classes around
    eg. pass int to a function if need the int class for reflection purposes (as defaultdict takes in c'tor)

x = 3
    variable declarations are not typed
x = 'hi'
    can be set to a totally different type later

2., 2e9, etc. for floats
    float('inf') and float('-inf') to get infinities
        these act like you'd expect (eg. infinity always stays infinity when add finite numbers)
    float('nan')
        stays nan no matter what operations you do to it
    math.isnan(x)
    all comparisons (==, <, etc.) with nan automatically False, even if both sides are nan!

0b, 0x, etc. prefixes for binary, hex, etc.

Variable Scope (unusual)
    if you read a variable, it automatically looks from the current scope up to global as usual
    but when you write a variable, it will only go up to the function's scope and then create a new variable if not there
    this also means that variables don't nest within blocks in functions in the normal way
        with statements, while statements, if statements, etc. can declare and set a value within them and it works fine
    to make a truly local variable within a block, you'd have to enclose it in another function scope
        eg. a helper function, or a nested/lambda function
    you can use the 'global' keyword to write to a global variable instead of making a new local variable
        global x
        x = 100
    similarly, you can use 'nonlocal' (which cannot be used for globals) to assign to a higher nested function variable
        nonlocal x  # x is from function containing this function
        x = 100
    an advantage of this is to be able to create and set variables with multiple possible values in branches more concisely
    NOTE: the variable is not "hoisted" like in JS with 'var'

there is no such thing as 'const' or 'final' to prevent modification
mutability and references
    l = [1, 2, 3]
    m = [l, l]
    l.append(4)
    print(m)   # [[1, 2, 3, 4], [1, 2, 3, 4]]

    def f(s):
        s.append(5)  # original affected
there is no such thing as a local static variable in Python
    some people create them like this:
        def f():
            if not hasattr(f, 'x'):
                f.x = 10
            else:
                f.x += 1
            return f.x
    other ways:
        default mutable args
        closures
        globals
        statics on a class

use \ at end of line to continue on next line
    may give you better result than how the formatter does long fluent API chains
also, wrapping in something like (), [], or {} causes a line continuation syntactically

python is very picky about indentation having to match and stuff like that

declaration order doesn't matter in the same way as other languages
    eg. within body of a function, you can use a variable that doesn't exist
    as long as it exists by the time the function is called elsewhere, it will work out
    this is the behavior of a scripting/interpretted language

you can check for the existence of a variable itself with:
    if 'x' in locals():  # note that we use string name of variable
      print(x)
    if 'x' in globals():
      print(x)

you can store variables on classes and functions that aren't defined in those places!
    this is what allows decorators to work
    it only works for stuff you define, not built-in stuff like integers and dictionaries
    eg. class MyClass:
          pass
        m = MyClass()  # note that you DO NOT USE NEW KEYWORD
        m.x = 10
        print(m.x)

    eg. def f():
          pass
        f.x = 10
        print(f.x)

    eg. def f():
            f.x = 5 # from within class, just use the name

Enums
    use the enum module (import Enum, auto, etc.)
    create an enum by either inheriting from Enum or creating an instance of Enum()
    in class version, each member is assignment
        either a value, or auto() call
    in functional version: Enum('Color2', ['RED', 'GREEN', 'BLUE'])

    ways to access:
        Color.RED
        Color(1)
        Color['RED'] # case sensitive
        for color in Color:
          print(color)
    
    each member is unique and singleton (with respect to both == and 'is')
        no matter how you access it above
    each member has a .name and .value (for the ordinal)

    enum class can have methods that can be called on the values (instances)

Default Values
    int() is 0
    float() is 0.0
    str() is ''
    bool() is False

[Collections]
collections in Python have literals but you can also use their explicit constructors (eg. list())
a lot of the stuff mentioned in list will apply other places
trailing commas are allowed in all literals (and even in function calls)

list
    x = [1, 2, 'hi']
    x[0] = 100 # mutable
    print(x[0])

    x.append(val)
    x.extend(y)
        destructively add elements of y to x (leaving y alone)
    x.insert(index, val)
    x.remove(val)  # first occurence
    x.pop()  # return and remove last item
    x.pop(index)  # return and remove item at index
    del x[index]
    del x[low:high]
    x.clear()
    x.reverse()
    x.copy() [shallow copy]
        x[:] is same thing
    x.index(item) for finding index of item in list from left side
        raises ValueError if not found
        linear search

    sum(l), max(l), min(l) top-level functions
        these also have variadic parameter versions
            eg. max(a, b, c)
        these can take `key` param to customize what you take the max and min by

    l.sort() destructive, sorted(l) non-destructive (copy)
    l.reverse() desturctive, reversed(l) non-destructive (copy)

    x = a + b + [1, 2] # concatenation (non-destructive)
        use a.extend() for destructive

    x == y # value equality implemented
    x < y # lexographic comparison implemented
    x = [0] * 100 # initialize with 100 0s

tuple
    x = (1, 2, 'hi')
    x = 1, 2, 'hi'
    # immutable!
    print(x[0])

    y = (1,)    # single value tuple syntax

    () = empty tuple

    and some of the stuff from list above
        inc. < ,==, and lots of stuff (immutable operations)

set
    x = {1, 2, 3} # mutable
    x.add(4)
    x.update([5, 6, 7]) # any iterable
    x.remove(7)  # raises error if not present
    x.discard(6)  # does not raise error if not present
    x.pop()   # remove and return arbitrary item
    x.clear()

    bitwise and minus operators overloaded to do set operations
    also set.intersection(s1, s2) static method
    also s1.intersection(s2) mutation

    NOTE: you cannot use {} as empty set because it's ambiguous with empty dict
        have to use set() constructor

    order is not guaranteed

    has == but not <
dict
    x = {'a': 1, 'b': 2} # mutable
    x['c'] = 3
    # NOT ALLOWED: x.c = 3
    print(x['c'])  # raises KeyError if not present
    print(x.get('c', 0))   # default value if not present
    del x['c']
    x.pop('b')
    x.clear()
    x.setdefault(key, value) # set value only if not present

    default iteration and membership checks are for keys
    you can be more explicit: x.keys(), x.values(), x.items() which are tuples of key and value
        these are sequences, not lists or tuples (doesn't usually matter)

    empty dict = {} or dict()

    order (of either keys or values) is based on insertion order

    has == (by keys+values), but not <

collections.deque
    double-ended queue
    can serve as an efficient stack, queue, or linked list
        because it's backed by a linked list
    like Java's LinkedList, since you're behind an interface that forces O(n) indexing
        it's not as efficient as implementing your own for certain operations
    otherwise, it more or less looks like a list with some extra members
        where certain operations under the hood have different performance characteristics
    
    append() goes to right side in O(1)
    appendLeft() goes to left side in O(1)
    pop() comes off the right side in O(1)
    popLeft() comes off the left side in O(1)

    you can construct with a maxlen optionally
        means if you add on one side and it goes above that len, items will fall off the other side
Stack
    use list with append() and pop()
    can peek with l[-1]
    or use collections.deque
    list one is probably ok since not recopying whole array, just growing and shrinking
Queue
    use collections.deque because inserting at beginning is super costly
    similar to list but popleft() and popright()
    still use append()

bytes
    x = b'abc=xff' # char and hex values

Cloning
    x = [*y] makes a shallow copy of list y into x
    x = {**d} makes a shallow copy of dictionary d into x
    x = list(y) is another way to clone (like a copy constructor)
        often take an iterable rather than specific type
        so that you can also do conversions
    copy module also can deep copy
Conversions
    x = set(l) # list to set (to dedupe)
    x = list(set(l)) # list to list with deduping
Unpacking
    x, y, z = collection
        works for tuple, list, or other sequence
        as long as the length of the collection matches the # of variables
    [x, y, z] = collection
        means the same thing as above (regardless of whether list or tuple)
    a[0],a[4] = collection
        the left side doesn't have to be a new variable or a variable at all
        it is just a general assignment  
Iteration
    for item in l:
        print(item)

    if an object is iterable, it will have an __iter__() method to return an iterator
    a generator function works as an iterator (supports the iterator interface)
    you can also make a class that has a __next__(self) method and return that
        python will call __next__() on the iterator as needed to get the next item
        it should raise StopIteration when there are no more items
    
    iterators can be directly used like this:
        it = iter(collection)
        while True:
            try:
                print(next(iter))
            except StopIteration:
                break
        
Membership
    if item in l:
        print('yes')

    all(item in small_list for item in l)
        a way to check if all items are in a collection
Comprehensions
    in general, look like the literal for the collection you're making
        with a for loop inside to generate the elements
    
    squared = [x**2 for x in nums]
    name_lengths = {name: len(name) for name in names}
    unique_vals = {x for x in nums}

    a special exception is with tuples and Generators
        gen = (x**2 for x in nums)
            this is a GENERATOR COMPREHENSION instead of TUPLE
        tup = tuple(x**2 for x in nums)
            this is a TUPLE COMPREHNSION (technically casting generator)

    nesting is done with 2 for loops inline
        [x**2 for row in matrix for x in row]
            this flattens the matrix and gets the squares of its elements as a flat list

    conditional comprehension with 'if'
        even_squares = [x**2 for x in nums if x % 2 == 0]

    generator comprehensions can also be used directly as function call params
        f(x**2 for x in range(10))
            f() will get called with 1 arg, the generator (which is iterable)

Negative indices
    -1 = last item, -2 = 2nd to last item, etc.
Slicing
    [2:5] # includes 2, 3, 4
    [:2] # includes 0, 1
    s[-2:] # includes last 2 items only
    s[:-1] # excludes last item
    s[start:stop:step] # full version (all 3 can be left off and/or negative)
        step -1 can be used to REVERSE the order
        but the start and stop are taken literally and might end up empty
        (ommiting start or stop is a special case where it will smartly adapt)
    you will get empty list for invalid range (not throw)

    for mutable collections, slices can be assigned
        eg. assign [] to clear that range
        eg. del operator on the slice to clear that range
        eg. assign a list to change that range (even on size mismatch)
    however, when stored as a variable, slices are COPIES of parts of the list

    NOTE: slice is actually a built-in top-level data type
        s = slice(2, 3)
Ranges
    the range() function works similar to slicing
    it returns an iterable that you can use with a 'for' loop
Sorting
    sorted(l, key = fn, reverse = False)
        fn should return value to sort by (can be derived/transformed arbitrarily)
        ascending by default, can also reverse it for descending
        the sort key will be used for sorting using the < operator
            if the sort key is a custom class (or no sort key so elements used), then __lt__ will be called
    the rough java equivalent is:
        < operator = Comparable<T> implementation
        key = Comparator passed in to override that
    in general, if a collection or method doesn't take a reverse parameter, you can use a key like this to reverse the order:
        key = lambda x: -x
        key = operator.neg
Shuffling
    use random module
Searching
    bisect module has bisect_left and bisect_right to do binary search on sorted collection
    bisect_left(sorted, item) uses < operator to look for leftmost occurence (and get index)
    bisect_right(sorted, item) uses < operator to look for 1st item after rightmost occurence
        can use as exclusive upper bound if want to take whole range
    both let you pass in low and high bounds of the search as well as a key
    if the index returned is len(sorted), then the insertion point is at the end (and item is not found)
        or in the case of bisect_right, the item you wanted might be the last thing in the array (special case)
    otherwise, you can tell if it was found or not by doing == on sorted[index]
Objects as keys
    any of the built-in collections are hashable and thus can be used as keys
    your own classes can be hashable if you implement the right stuff too
Implementing Objects for Collections
    == operator is used for membership tests
    < operator is used for sorting and binary searching
    hash() is used for dict insertion
    hash() and == are used for dict retrieval
    repr() is used for printing collections (trickles down to elements)
    str() is used for converting collections to string (trickles down to elements)

Sorted containers (from 3rd party sortedcontainers library)
    (avialable in Leetcode for solving problems)
    SortedList
        acts like a list but keeps item in sorted order (duplicates allowed)
            O(logn) lookup both by index and by value (due to tree)
        you can retrieve items by index, but you cannot add them by index (even implicitly)
            eg. cannot do append() or extend() because they add to end
            eg. cannot do insert() or a[i] = val
            eg. can't do reverse() [but can do reversed()]
        like a normal list, it is iterable, can check membership, etc.
        SortedList(seq, key = None) constructor
            passing a key actually gives back SortedKeyList which is subclass
        < operator used for items
        bisect_left(val) and bisect_right(val) methods
    SortedKeyList
        a SortedList subclass that uses a custom key function
        adds bisect_key_left() and bisect_key_right() methods too
    SortedSet
        acts basically like a normal set, but when you iterate, the items are in sorted order
        < operator and/or key in c'tor
            hash() also used
        [] to get by index (inc. -1)
    SortedDict
        acts basically like a normal dic, but when you iterate, the items are in sorted order (by keys)
        < operator and hash()
        uses SortedList internally, so methods like bisect_left(key)
        use peekitem(index) to get (key, value) by index (inc. -1)

Priority Queue/Heap
    the heapq module (built-in) provides a priority queue/minheap implementation
        but instead of a class/type, it is a set of functions that operate on normal lists
        if you wanted it to be a class, you could make a class to wrap it easily
    to use a custom class, you would implement the < operator as usual
    to make it a maxheap, you have to use a hack because there is no way to use a custom key
        eg. negate the value on push, then negate it again on pop
    
    start with [] (an empty normal list)
        or take a list with values and call heapify(l) to turn it into a heap (heap sort it)
    heappush(l, val) puts a value into the heap
    heappop(l) gets the smallest value from the heap and reduces its length by 1
    heappushpop(l, val) = push then pop
    heapreplace(l, val) = pop then push

Immutability
    list -> tuple
        independent copy
    set -> frozenset
        independent copy
    dict -> types.MappingProxyType
        view on the original

Thread Safety
    in general, read operations on collections are thread-safe, but write operations are not
        and read during write is not
    this applies to almost all the built-ins as well as sortedcontainers
        also applies to heapq
    one exception is collections.deque, which is designed to be thread-safe on the two ends
        but not in the middle
    to add thread safety, use locking

Swapping
    a,b = b,a # right side makes tuple, left side unpacks it

Not Available:
    persistent slice that responds to changes in original and/or can be used to modify the original
    a lot of the optimization methods in set and dictionary
    in-place compuatations on dicts
        which probably doesn't matter if using the O(1) version anyway
    built-in sorted dict and set (use sortedcontainers library)
    directly making maxheap (have to use a hack)
    special types for bitfields, bit sets, enum sets, enum maps, etc.
    string builder (see in Strings)
    case insensitivity in strings
    tree/graph directly by nodes (implement yourself easily with classes)
    struct/value type (though @dataclass and NamedTuple has that kind of goal)
    proxy pattern immutable wrappers
    proxy pattern synchronized wrappers

[Strings]
# for comments
no null termination (that is just a C++ thing)

'' and "" are same (and allow to use other unescaped)
''' and """ are multi-line strings
r'' and r"" are raw strings
f'' and f"" and f''' and f""" are f-strings

immutable (modifications return copy)
    you can get a mutable copy as a list of chars with list(s)
        make back into string with ''.join(c)

strip() to remove leading/trailing whitespace
    lstrip(), rstrip()
replace(old, new)
upper(), lower(), title()
split(delim) -> gets list of strings (similar logic to Java but not regex)
    split() -> split by whitespace
delim.join(l) -> join a list by a delim string (method of str)
startswith()
endswith()
isascii(), isnumeric(), isalpha(), isalnum(), isupper(), etc.

find(), rfind(), replace(a, b, count=None), removeprefix(possiblePrefix), removesuffix(possibleSuffix), 

s = s1 + " " + s2

if substring in s:   # substring search
    print('present')
s.index(substring)   # explicit search
s.count(substring)

formatted strings
    NOTE: with any of these, you can store the template as a variable in itself
        though in an f-string, that would already be evaluated
    
    f'Hi, my name is {name} and I am {age + 10} years old.'
        name and age should be variables in the encosing scope
    'Hi, my name is %s and I am %d years old' % (name, age)
        this is the old-fashioned way, replaced by f-strings above
        for single value, no () needed
    'Hi, my name is {} and I am {} years old.'.format(name, age)
    'Hi, my name is {name} and I am {age} years old.'.format(name=name, age=age)

    {:.2f} (example of format specifier - in this case float with 2 places after decimal point)
        or in the old style %.2f
        see Java notes for rundown of the specifiers (%02d, %.2f, %15s, %-15s, %s, %d)
    format specifiers are pretty permissive about type mismatches

string as collection
    len(s)
    s[0], s[-1], s[1:3]
        none of these support assignment
        slicing gives another string
    for c in s:
        print(c)
    l = list(s)   # mutable copy as list of characters
    because str is iterable, things like sorted(s) will work too

    NOTE: character is not a type - it is just a string of length 1

string builder
    there is no StringBuilder-like class in Python
    the pythonic way to efficiently append to a string is to build a list and use ''.join() on the list
        could even use a list comprehension to do it all inline

conversions
    int(s) will throw ValueError if not a proper integer string
        isnumeric(s) can be used to check it first
    str(n) will use the __str__ method of the object (simple for types like int)
    repr(n) will use the __repr__ method (simple for types like int)
    print(n) will automatically use __repr__

regular expressions
    see 're' in 'stdlib' section

'string' package
    for getting list of ascii letters, punctuation, etc.

Encoding
    python strings (in python 3) are unicode (utf-8)
    to convert a string to bytes, use s.encode('utf-8')
    to go the other way, b.decode('utf-8')
    previous versions of python had a lot of complications about unicode but not anymore

Line endings
    the different system ones all becomes \n on read
    \n becomes proper system one on write
    there are params to customize it in open(), read(), write(), etc.

Case insensitivity
    not built-in like most languages
    commonly done by converting tolower() for comparison
    or use re.IGNORECASE with regex

Extra copies
    you can assume operations that take portions of strings make copies
        for same reason as Java - don't want to keep big one around for small one
    but it's possible under the hood some optimizations are done you don't know about

Ascii Values
    because there is no char type, you can't do this: s[0] - 'a'
        instead, do this: ord(s[0])
        the opposite is: chr(n)

[Operators]
mostly the same as C-like languages with some exceptions
    additional operators like // and ** also support assignment (//= and **=) like others

missing
    ++ and --
    safe navigation
division
    // behaves like / in c-like languages (floor division)
    / becomes a float instead!
exponentiation
    a**b means a to the b power
boolean
    use words and, or, not instead of operators &&, ||, !
    eg. a and not b
bitwise
    unlike booleans, these use the usual symbols (&, ^, ~, >>, etc.)

presence in collection (boolean)
    a in somelist
    b not in somelist

equality
    a is b
        True if same numeric value or same reference
        same as == in Java
    a is not b
        opposite of a is b
    a == b
        value equality
        more like .equals() in Java
    a is None
        considered better than doing a == None

truthiness
    since types aren't bound, there is a lot of flexibility for checks on variables
    the following values are falsey in conditionals:
        None
        False
        not True
        0
        ''
        []
    the rest are truthy
    you can disambiguate None from [], for instance, using is None first

coalescing
    'and' operator actually returns last item seen in short-circuiting
        which could be truthy or not
    'or' operator actually returns last item seen in short-circuiting
        which could be truthy or not
        this can be used for coalescing:
            y = x or 0
                if x is truthy, y = x
                if x is falsey, y = 0
    the short-circuiting and condition logic are controlled by __bool__ conversion
    'not' will just convert to boolean right away

ternary
    no ? : like in C-like languages
    instead it looks like:
        a if a > b else b

collections
    [1, 2] * 3
        [1, 2, 1, 2, 1, 2]
    [1, 2] + [3, 4]
        [1, 2, 3, 4]
    [*mylist]
        spread operator (in this case copying mylist)

del
    x = 5
    del x
    print(x)    # Error
    del x,y   # delete multiple

    l = [1, 2, 3]
    del l[1]  # l = [1, 3]
    l = [1, 2, 3]
    del l[1:] # l = [1]

    del mydict[key]

hasattr(obj, name) to see if an attribute exists on an object
    useful if you're going to be hiding variables in functions, for instance
    eg. hasattr(x, 'count')

[Control Flow]
; isn't required, but it is a way to fit multiple statements on one line
    eg. for shell one-liners
statements that are normally multiline can be single-line and can use ;
    eg. for i in range(10): print(i); print(i)
        this will print each number twice in a loop

pass
    a noop statement
    eg. for an empty loop or function body (because required to have something there)

if a < b:
    print(a)
elif b < a:
    print(b)
else:
    print('same')

while a < b:
    a += 1
    print(a)

for element in l:
    print(element)

for index, element in enumerate(l):
    print(index, element)

for x, y in l:
    # each element of l is a tuple which is unwrapped here

for i in range(10):
    print(i) # 0, 1, 2...9

break and continue supported as usual

match term: # like switch in another language
    case 'bla1':
        print('bla1')  # no fall-through, so no break needed
    case 'bla2':
        print('bla2')
    case _:    # like default in another language
        print('default')

try:
    someCodeThatMightThrow()
    raise MyThrownException('I threw')
    print('this code is unreachable')
except ZeroDivisionError:
    print("division by zero!")
except MyThrownException as e:
    print("Error: " + str(e))
except (OtherError1, OtherError2) as e:
    print("Either OtherError1 or OtherError2" + str(e))
finally:
    print("this will run no matter what")

Exception is the usual common base of exceptions
    raise Exception('summary', 'details')
    e.args
    str(e)
    traceback.print_exc() [don't need to pass in e]
        gets callstack and prints to stderr
some built-in errors:
    Exception
        the most common base
    KeyError
        eg. dictionary doesn't have key
    ValueError
        eg. int() on a string that is not an integer
    AttributeError
        attribute (method or variable) not present on object
    StopIteration
    IndexError
        eg. subscript out of range
    ZeroDivisionError
    FileNotFoundError
    KeyboardInterrupt
        triggered by ctrl-c
    NotImplemented

with open('myfile.txt') as f: # context management
    contents = f.read()
print(contents) # notice contents variable still lives (as does f, but in a closed state)

with open('myfile.txt'):
    print('not necessary to keep variable - context is still active')
with open('myfile.txt') as f, open('myfile2.txt') as g:
    print('2 contexts at same time!')

asserts run as long as you don't run with -O (optimized) flag
    assert condition
    assert condition, message

[Functions]
def f(a, b, c):
    return a + b + c
x = f(10, 20, 30)

Overloading
    true overloading does not exist
    if you define the same name twice, the later one replaces the earlier one, even if params are different
    you have to simulate overloading with default args and keyword args
Tuples
    return (a, b)
        use tuple to return multiple values from a function
    a, b = f(x)
        tuple unwrapping in-place
Default args
    def f(x, a=0, b=0):
        pass
    f(10)
    f(10, 20)
    f(10, 20, 30)

    def f(a=g()):  # not a "compile-time constant"
        return a

  you cannot ommit args if they don't have defaults
  you cannot add extra args beyond what's in the list
Default Mutable args
    this is a weirdness of Python
    when a default value of an arg is a mutable type like a list or dictionary
        then modifying it within the function will modify the default (because it's cached that way)
    eg. def f(x, a=[]):
            a.append(x)
        # every call to f without an a arg will grow the default a arg
        # if you call with an a value, then call again without one, the cached one will come back
Keyword args
    using same f as "Default args" above
    f(10, b=1000, a=700)
    f(10, b=10)

    f(a=a)
        something you commonly see when delegating args
        Python knows the difference between the left and right side of this
Default return
    when a function doesn't have a return statement, or reaches the end without returning
    it returns None (which you can ignore to make it simulate void in other languages)

Spread (star and double-star)
    f(10, arg=100, *l, **k)
        l = positional args expanded in-place
        k = keyword args expanded in-place
        a way to forward variadic args
            in addition to using collections to store args
Rest/Variadic (star and double-star)
    def f(x, a=10, *args, **kwargs):
        pass
    args becomes a list of positional args passed in besides the named ones
    kwargs becomes a dictionary of keyword args passed in besides the named ones

Generators (lazy)
    def f():
        yield 1
        yield 2
    print(list(f())) # evaluate whole thing

    def g(seq):  # lazy sequence in terms of another lazy sequence
        for item in seq:
            yield item**2
    print(list(g(f())))

    def h():
        yield 0
        yield from f()  # expanded in-place (like * operator)
        yield 3

    def i():
        x1 = yield 0
        x2 = yield x1 * 10
        yield x2 * 20

    # two-way communication
    gen = i()
    next(gen) # 0
    gen.send(100)
    next(gen) # 1000
    gen.send(1000)  # x2 becomes None if you don't call this
    next(gen) # 10000
    next(gen) # raises StopIteration
    # you could use the gen.send() values to actually detect when to stop returning (eg. break a loop)

    NOTE: generators aren't subscriptable, so you have to use itertools.islice() to slice/limit them
        you also can't directly take len() for the same reason [maybe do len(list(gen))]
        another way to count the items in a generator without making a temporary list:
            sum(1 for _ in gen)
        both methods consume all the items from the generator

    generators are STATEFUL
        if you consume 1 item, then convert to a list, that will be missing the 1st item
        to start over, you have to call the generator function again to get a new generator
        you can store the result of the call as a variable to keep the state

Extension Methods
    not directly supported, but easy to do
    just make a function that takes a self param first
    then assign to the class itself (or to an instance if you want)
        eg. MyClass.my_method = my_method
    then you can call it like any other method

recursion
    function calls itself by name
    no TCO
    lambda call themselves by name too, even if assigned a variable
        eg. f = lambda # bla bla bla
            the lambda can call f() here

not supported: macros, inline, ref and out params

[Functional]
f = lambda: print('hi')
g = lambda x: x**2
h = lambda x, y=10, *args: x + y + max(args)
def myfunc(x):
    print(x)
x = myfunc
    a function name is just a variable assigned with a function object
    it can be reassigned and they both point to the same thing

f()
print(g(10))
print(h(10, 20, 30, 40))
x(10)

nested functions are allowed and can be used like lambdas too
    def f():
        def lam():
            print('hi')
        return lam
    f()()

    NOTE: this is the only way to do multi-statement lambdas in Python
        there is no way to have {} around a body like in other languages

ignored params: _, __, etc.
    you can use these in a function or a lambda as params
    they are technically real names and can be used as such
        but by convention, they are ignored params
        usually to make args match without losing readability
    eg. for _ in range(10):
            print('hi')   # print 10 times, don't need index
    eg. a, _ = f()  # unwrap without length error, but only need 1st thing

functional operations
    map
        seq = map(lam, l) # lazy sequence
        print(list(seq))

        NOTE: you can often do LIST COMPREHENSIONS instead
    reduce
        val = functools.reduce(lam, l, seed)
        print(val)
    filter
        seq = filter(lam, l)
        print(list(seq))

        NOTE: you can often do LIST COMPREHENSION with an 'if' inside

    zip
        multiple lists to list of tuples
            seq = zip(l1, l2, l3)  # variadic
            for i1, i2, i3 in seq:
                print(i1, i2, i3)
        list of tuples to multiple lists
            l1, l2, l3 = tuple(zip(*seq))
        parallel iteration
            for i, j in zip(is, js):
                print(i, j)

        NOTE: zip is lazy like the other functional operations
        NOTE: zip will truncate to the shortest of the input sequences
            to pad to the longest with a fill value, see itertools.zip_longest
        
    all
        all(seq) = True if no elements are falsey (inc. if empty)
    any
        any(seq) = True if at least one element is truthy (false if empty)

    a lot of operations that are explicit calls in other languages are handled by Python syntax already
        groupBy, flatMap, etc. can be done just by COMPREHENSIONS
        limit and skip can be done by SLICING (but need to use islice if itertools sequence or generator)

    partial application done by functools.partial()

itertools
    see stdlib section below
    itertools lets you do various stream operations for declarative lazy sequence processing
    an alternative to writing your own generator comprehensions and such
        or along with it, such as this comprehension: (x**2 for x in itertools.repeat(10))

Closures
    python closes BY REFERENCE over variables by default
        for i in range(10):
          yield lambda: print(i)
        This will print 9 over and over (10 times)
            because i is captured by reference even though it's primitive!
            then the lambdas are returned all closing on the same variable that continues to live
        This happens with nested functions too, not just lambdas!
    in order to close BY VALUE instead, you need a lexical capture point (a function call)
        def make_lambda(x):
          return lambda: print(x)
        for i in range(10):
          yield make_lambda(i)
    the good part about this behavior is you can modify the state
        eg. increment a counter, iterate lines in a file, etc.
    just remember that when it's a parameter, it's by value and when it's implicitly captured, it's by reference

Class Method References
    m = MyClass()
    map(m.f, l)
        method f of class MyClass is BOUND to instance m
        map() will call it like a pure function, and m will automatically be passed as self
        NOTE: if you see lambda x: m.f(x), you can probably just replace that with m.f
    MyClass.f(m)
        method f of class MyClass is UNBOUND, so you have to pass an instance as the first arg
    you can treat a class itself (or a variable referencing it) as a callable function for construction
        constructor = MyClass
        m = constructor(10)
Operator Module
    functions like add, mul, neg, lt, etc.
    so that you can treat operators functionally too
Commonly Used Method References to Replace lambdas
    str.lower
    list.append
    int
    len
    sum
    operator.add, operator.mul, etc.

Chess Queen Attacks
    to avoid writing 8 nasty for loops
    create generators for the 8 directions of infinite movement from a point
        1-dimensional generators zipped together
    itertools.takewhile() based on boundary validity and problem criteria
    len(list(seq)) after doing those filters (and sum the 8)

    the iterative alternative is 8 for loops, each with either nesting or hardcoding one of the variables
    then each for loop has to check other criteria (like if hits obstacles) and bail
    then just increment count each non-bailed iteration
    conceptually the same but uglier, more repetitive, and have to craft each loop w/ copy/paste

Fibonacci:
    1. use functools.cache to memoize a recursive function
    2. use a generator that remembers previous 2 values in a rolling window
    3. use a list and compute item by item

IIFE
    (lambda x: x**2)(5)

not supported:
  - pre-defined function types (for type hints only can use Callable)
  - special findfirst and findany calls

[Classes]
like with functions, a class object can be assigned to variables and still act as the class
    eg. OtherClass = MyClass
        o = OtherClass()

'self'
    similar to how you have to put 'this' everywhere in JS, you need 'self' to access instance members
    in addition, you have to explicitly pass 'self' as the first param of instance methods!
Inheritance
    uses a prototype-like inheritance (similar to JS)(but no prototype member)
        if an attribute (either variable or method) is directly set on an instance, that is used
            allowing you to add and replace functions and variables at runtime
        if it is not directly set, it is forwarded to the class
            the (technically) class/static members are then returned/used
        if not set on the class, then the base classes (from left-to-right) are checked in DFS order
            if not found in the hierarchy at all, then AttributeError is thrown
    a key thing to note here is that instances dynamically see changes to their classes
        it is not clone stamped at the point of instantiation
        but changing superclass won't affect instance if subclass defines that symbol
            WARNING: if you modify an object (instead of rebinding a variable) in subclass,
            you are changing the static class!
                eg. appending to an array defined in the class body affects all instances
                    this is a sharp corner you don't see in other languages - be careful!
    no 'virtual' keyword - everything is virtual (like Java) due to how the lookups work
        just using the same name overrides already
        both for variables and functions
    static methods act this way as well
        the difference is that they don't have to have self as the first param
    also note that everything is by name only, not by args or type
        because no overloading in python

    diamond inheritance
        ambiguity resolved by taking first thing from left in class's inheritance list (MRO)
        a result of how attribute lookups work

    unlike other languages, base c'tor is only automatically called if derived class has no c'tor!
        can explicitly call it like below though
object
    the root of the type system is called 'object'
    even primitives inherit from object
    the default hash function returns a different hash for each instance
        which makes it useful for unique opaque dictionary keys and such
    object can be directly instantiated too
        eg. to use as opaque key or something
Access Modifiers
    apply within classes as well as top-level things in modules
    default = public
    _ = protected (or semi-private)
        only by convention (not enforced)
    __ = private
        enforced by NAME MANGLING at runtime
            though the mangling is predictable and defeatable
    __somename__ = not an access modifier but a special value with special meaning

    the "we are all robots" case (instances accessing each others' private variables) works here

class MyClass:
    static_var = 10
    other_var = static_var  # no self needed because not in a method

    def __init__(self, instance_param):
        self.instance_var = instance_param

    def f(self, x):
        print(x)
        print(self.instance_var)

class MyDerivedClass(MyClass):
    def __init__(self, instance_param):
        super().__init__(instance_param)
        self.derived_var = 10

    def f(self, x):
        print('inherited!')
        print(x)

    @staticmethod   # special attribute for static method
    def g(x):       # no self needed for static
        print(x)

    @classmethod
    def h(cls):     # class passed as arg
        print(cls.static_var)

class MultipleDerivedClass(BaseClass1, BaseClass2):   # no restriction
    pass

m = MyClass(x)  # self passed implicitly (no 'new' keyword)
print(m.static_var)  # this is actually a copy now
print(MyClass.static_var)
print(m.instance_var)
m.f(100)

MyDerivedClass.g(100)
MyDerviedClass.h()

Interfaces
    python doesn't have interfaces - everything is duck-typed
    as long as the things referenced or called by the caller are on the object, it works
    you could throw NotImplemented from a method to make it "abstract"

Abstract Base Class
    you could just have methods throw NotImplemented to make an ABC
    there is also an abc module to make it a little more automatic (but not much)
    hooks into the internals of the class to make it act like a language feature
    see details in [stdlib]

Special Class Members
    certain members can be present on your class to implement certain behaviors in python

    __init__(self, ...) = c'tor
    __del__(self) = d'tor

    __str__(self) = converting object to string
        eg. str(m)
    __repr__(self) = string representation for debugging
        eg. print(m)

    __len__(self) = length of the collection
    __iter__(self) = iterator class or generator function for iterating elements
    __contains__(self, item) = membership check

    __getitem__(self, index) = index operator (index could be anything you want)
        NOTE: can use isinstance(index, int) and isinstance(index, slice)
            then you can have code to handle indexing and slicing
            using a[2:3] automatically passes in a slice(2, 3) to the __getitem__ function
            using a[2,3,1:4] automatically passes a tuple to the __getitem__ function
                in this case a tuple of 2 integers and a slice
    __setitem__(self, index, value) = setting value by index
    __delitem__(self, index) = deletion of value by index
        
    __getattr__(self, name) = for reading values from an instance with . operator (as if fields)
    __setattr__(self, name, value) = for dealing with assignment via .
    __delattr__(self, name) = for deleting with deleting attributes via .

    __getattribute(self, name)__ = generic attribution retrieval

    __call__(self, ...) = call operator - for calling object as if it is a function
    __enter__ and __exit__ for context management

    __eq__(self, other), __lt__(self, other), __add__(self, other), __sub__(self, other), __and__(self, other), etc.
        implementations for operators like ==, <, +, -, etc.
        a lot of operators have an r version for when your type is on the right side instead of left
            eg. __radd__(self, other)
            first, python will call the class on the left side and only call you if they throw NotImplemented
        if operating with a type you don't know how to deal with, you should throw NotImplemented so the other type gets a chance
    __bool__(self), __int__(self), etc. for type conversions
    __hash__(self) = for hashing (eg. in dict)
        if object has multiple fields, make them a tuple and hash that
        eg. return hash((self.field1, self.field2))

    NOTE: a lot of these methods have top-level functions without underscores you can call to invoke them conveniently
        eg. instead of m.__hash__(), you can call hash(m)
        eg. hash(), str(), len(), iter(), etc.

    even more advanced ones like __new__ to hook higher-level construction and do crazy things
        eg. things like @dataclass and ABC

Constructors
    there is only one method of each name due to lack of overloading in Python
    so if you provide multiple __init__, only the last one will stick
        the others will be uncallable
    thus, there are no delegating constructors within same class
    if you don't provide __init__, then the object can be instantiated with no args
    a subclass constructor does not automatically call the base class constructor!
        you will usually want to do that explicitly
        unless you don't provide one on the subclass
            then it automatically calls the default base constructor

Properties
    class Circle:
        def __init__(self, radius):
            self.radius = radius

        @property
        def diameter(self):
            return 2 * self.radius

        @diameter.setter
        def diameter(self, value):
            self.radius = value / 2

        @property
        def area(self):
            return 3.14159 * self.radius**2

    circle = Circle(1)
    print(circle.diameter)
    circle.diameter = 4
    print(circle.diameter)
    print(circle.area)

Dataclass
    (for simple POD types)

    from dataclasses import dataclass

    @dataclass
    class MyClass:
        x: int  # required constructor arg
        y = 20  # optional constructor arg

        def f(self):
            return self.x + self.y

        # __init__ defined for you
        # has the member fields in order
        # ones with values are optional

    m = MyClass(10)
    print(m.f())

    NOTE: MyClass.x is a type hint, not an attribute
        at least until @dataclass wraps it in a class that has the attribute for real
        type hints like this go into the __annotations__ field of the class
        if you didn't have the decorator, and did MyClass.x, it would throw AttributeError

    dataclasses also implement __repr__, __str__, and __eq__
        does not get __lt__, etc. by default
    it also gets __hash__ if you pass frozen=True in the decorator
        which makes it a readonly data structure after construction

nesting
    clases, functions, etc.  can be nested inside each other w/ no issue
    nested classes don't inheritently have access to each others' private data
    you can define a class inside a function and return an instance of it
        this is a way to do anonymous classes

not supported: partial classes, friend classes (can use _ variables), sealed/final
a variable and method of the same name cannot exist because methods/functions are variables in Python

[Generics]
because python is dynamically weakly typed and duck-typed, it does not have generics
any collection is a generic, with no constraints at all on elements (can mix and match however)
however, you can somewhat simulate it at least in type hints using Typing.TypeVar (for type checking purposes only)

[Imports/Exports/Modules]
import os
import sys
print(os.path)

import os, sys
print(os.path)

import os as operating_system, sys as system
print(operation_system.path)

from os import path
# avoid having to always use fully-qualified path
# works for module as well as symbols within module
print(path)

from os import path as os_path
# this works for renaming modules as well as symbols inside modules
print(os_path)

from os import *
# import all symbols (usually not recommended)

anything can be at the top-level of a module (variables, classes, functions, etc.)

python3 myfolder/myfile.py
    direct execution
    search path not used
python3 -m myfolder.myfile
    module execution
    search path used
        each . is a subfolder, but the root can be anywhere in search path
        this allows you to execute libraries and installed dependencies this way too
        current working directory, PYTHONPATH variable, installed packages, etc.
python3 -c 'import myfolder.myfile'
    execution via import (within one-liner shell command)
python3 -c 'from myfolder import myfile; print(myfile)'
    multiple statements within 1 line

__name__
    '__main__' for direct execution or module execution
    name of the module itself if executed via an 'import' instead
        either via -c or via executing another  .py file

    very common to see if __name__ == '__main__' at top-level to distinguish library vs. script execution
        eg. could run unit tests for script version

__file__
    path of the .py file (the current one, not the top level one)

sys.argv[0]
    the top-level thing being executed (instead of current file)
    for direct and module, it is the current file, but for imports, it's the thing executing at the top
    for one-liners that import the file, it's just -C
sys.argv[1:]
    args to the top-level script (passed to python3)
    things like --flag are passed exactly like that as 1 arg
    things like -n 5 are passed as 2 args exactly like that one after the other
argparse built-in library can help parse the arguments
common pattern in pip packages to have a dictionary of command name to function to call
    then use 1st arg as command and forward args to the function with * operator

package
    if a folder has an __init__.py file, it is a package
    a package will import subpackages (usually relative with . and ..)
        eg. from .deepsubfolder import deepmodule
            then it's available as mypackage.deepmodule to the caller
    packages can be in a hierarchy of other packages
    importing from subfolders directly won't work - you have to import the package and let __init__.py set it up

name clash issue
    if you name a file the same as an api you're calling, it will try to import itself
    the symptom of this will be that the import is empty
    another place this comes up is if a folder is named after a script (causing ambiguity)

file/module name limitations
    dash is not allowed (because not allowed in identifiers)
    . is not explicitly disallowed but is ambiguous with folder and thus won't work

[stdlib]                            
print(x)
print(x, y, z)
    print 3 values with space in between on single line
print() for newline only
print(x, y, z, sep='\t', end='')
    overriding sep and end
    in this case, tab between x, y, and z, and no newline at end
print("x:", x)
escape sequences for color (have to escape back again)
    alternatively, use a library like termcolor
print('text', file=sys.stdout)  # can choose what file to print to (default is stdout)
    can use sys.stderr also

text = input('Please enter a number: ')
    prompts user and reads what they enter (until and excluding the newline)
    you can loop for input if you want, and use a combination of a user entered value and KeyboardInterrupt (ctrl-c) to break
num = int(text.strip())
    example of reading numeric value
vals = [int(item) for item in input('Please enter numbers: ').strip().split()]
    example of reading structured input

open() for file I/o doesn't need an import
    with open(path, 'r') as file:  # open for text read
        for line in file:          # iterate lines (inc. newlines)
            print(line.strip())    # strip whitespace
        
        lines = file.readlines()   # list of lines (inc. newlines)

        content = file.read()      # read whole file as single string
other open modes include:
    w = write (use write() method which does not write a newline) (use \n)
    a = append
    b = binary (bytes instead of str)
    eg. 'wa', 'wb', 'rb', etc.

math
    isnan(x)
    isclose(x, y)
        for float near-equality
    sqrt(x)
    floor(x)
    exp(x) [e to the x]
    log(x) [natural log]
    log(x, base)
    radians(angle)
    cos(radians)
    fabs(x) for float absolute values
        abs(x) is a top-level function that also works on floats, integers, etc.
        math.fabs() returns floats even if given integers while abs() maintains the input type
    pi = constant
    e = constant
operator: functions for passing operators around functionally
    add
    mul
    not
    lt, gt, eq
    neg
    truth
    is
    abs
    getitem, setitem
statistics
    mean(l)
random
    seed(value)
    randint(inclusiveLower, exclusiveUpper)
    random() [float from 0 to 1, not including 1]
    shuffle(l) [destructive]
    gauss(mean, stddev)

    Random() to make an independent random number generator instance
        call with methods like the above
        eg. to put on thread-local storage

bisect
    bisect_left(sorted, element)
    bisect_right(sorted, element)

functools
    reduce(fn, l, seed) [but map and filter are in global/default namespace]
    partial(fn, *args) used to get a function with args bound so you don't need to pass them
    partialmethod(method, *args) is like partial() but for class methods (ignores and preserves 'self' param)

    @cache
        memoize a function automatically
        eg. make a recursive function and just put this decorator to make it not waste effort

    @total_ordering
        if a class has at least __eq__ and one of the comparison operators
        then it will generate the rest of the comparison operators

itertools
    chain(seq1, seq2)
        concatenates multiple sequences lazily into one sequence
    cycle(seq)
        lazily repeats a sequence forever
    repeat(val, n)
        lazily repeats a value n times
    repeat(val)
        infinite lazy sequence
    count(start)
        infinite lazy sequence of consecutive integers
    count(start, step)
        same but with a step other than 1
    combinations(seq, length)
        all combinations of 'length' elements of seq 
            only unique combinations, with same element not being paired with itself
    permutations(seq)
        all possible permutations of sequence
    product(seq1, seq2)
        cartesian product of all combos of elements of the two sequences (as tuples)
    pairwise(seq)
        tuples of overlapping pairs
        eg. [1, 2, 3, 4, 5] -> [(1, 2), (2, 3), (3, 4), (4, 5)]
    dropwhile(lam, l)
    takewhile(lam, l)
    zip_longest() = like zip but lets you pad short sequences to match longest sequence length

    the values returned by these are iterable sequences, but not indexable/slicable
        you can use islice() to lazily slice them

    islice(seq, count)
        for limiting/slicing infinite sequences that aren't subscriptable
        eg. generators and things returned by itertools calls
    islice(seq, start, stop)
    islice(seq, start, stop, step)

collections
    defaultdict: dictionary with default values for keys not present
        d = defaultdict(int)  # specify the type in c'tor
        print(d['notpresent']) # 0
        d['notpresent'] += 1 # becomes 1
        trying to get or set the key makes it present from then on
    Counter: count dictionary
        counts = Counter([1, 1, 2, 3, 3, 3, 4])
        print(counts[1])  # 2 because there are 2 1s
        print(counts[0])  # 0 because not present
        print(counts.most_common(2)) # [(3, 3), (1, 2)]
        print(counts.most_common(1)[0][0]) # 3
        print(sum(counts.values()))  # total count
    namedtuple: tuple with named fields (like a struct)
        acts like a normal tuple and a class at same time

        Point = namedtuple('Point', ['x', 'y'])
            give the internal names of the class to create and the fields
                all as strings
            pass a 'defaults' keyword arg with a list to provide default values
        p = Point(10, y=100)
            takes positional and keyword args

        p[0]
        x,y = p
        p.x
        p.y

        implements ==, <, str(), repr(), hash(), etc. for you
            acts like a normal tuple

    deque: double-ended queue
        more efficient than regular lists for stack and queue operations

    abc (abstract base classes)
        Callable = type annotation for a fn
        Sequence = type annotation for a generic sequence
heapq
    heapify()
    heappush()
    heappop()
    heappushpop()
    heapreplace()

abc: abstract base classes
    ABC: inherit from this to make the class not instantiable directly
    @abstractmethod: mark methods that subclasses must override
        if a class inherits your class and overrides all these, it is concrete

os
    path
        expanduser('~/subfolder') -> '/home/username/subfolder' for instance
        abspath(path)
        relpath(abspath, relativeToPath)
        dirname(path) -> parent folder of file or folder
        basename(path) -> remove extension if present
        splitext(path) -> split into basename and extension parts
        join()
        exists()
        isfile()
        isdir()
    walk(path)
        pre-order DFS (alphabetical) of folder tree starting at root
        iterable of tuples of 3 items so you can do this
            for dirpath, subdirnames, filenames in os.walk('.'):
              print(dirpath, subdirnames, filenames)
        paths are made to look like what you passed in (relative of absolute)
        what you see in subdirnames, you are about to go into on a future iteration
            but the current iteration is your chance to deal with the files themselves
    listdir()
        list files and folder (by name) in current working directory (or given directory)
    getcwd()
    chdir()
    mkdir()
    makedirs() [to make multiple levels]
    rmdir() [must be empty]
    environ = dictionary of environment variables (no $)
        can be used to get/set in current shell instance
    getenv(varname, default)
    putenv(varname, value)

shutil = higher-level OS file system functionality than os
    copy()
    move()
    rmtree() [doesn't require empty]
    make_archive() [to make zip file]
    unpack_archive() [to unzip]

sys
    stdout
    stderr
    stdin

    argv = list of cmdline args (element 0 = the top-level call such as the python file)
    exit(code) = exit whole program with exit code
    stdin, stdout, stderr
    path = module search path (list of strings)
        can append to it to add to the search path dynamically before importing a module
    platform = OS info
    version = Python version
    modules = currently imported modules

glob
    glob('**/*.py')
        recursive list of all python files in all subfolders in current working directory

traceback
    print_exc()

re
    use rawstrings to delimiante one level of \
    methods tend to take a regex string first
        because you can use re.compile(regex) to compile
        then methods on the compiled regex will look the same but drop first arg
            technically just calling them as instance instead of static
    
    named capture groups look like this in python: "(?P<name>text)

    sub(regex, replacement, text) -> returns new text with all replacements made
        can refer to capture groups as \1, \2, etc. (or \g<1>, \g<2>, etc.)
        can refer to named capture gruops as \g<name>
        both of these differ from other languages where you use $
    findall(regex, text) -> list of all substrings matching
        if regex has capture groups, you get a tuple of capture group texts for each match instead
    finditer(regex, text) -> like findall, but more info
        match.group(numOrName) to get capture groups

    compile(regex) -> can call the methods above without first arg
        pre-compiled for efficiency on multiple texts

    modifiers like re.MULTILINE go as last arg in compiling or in method calls
        re.MULTILINE = treat ^ and $ as per line
        re.DOTALL = include newlines in .
        re.IGNORECASE = case insensitivity
        combine with bitwise | operator

    ** see Regex Syntax spreadsheet **
string
    ascii_letters = collection of all ascii letters
    punctuation = collection of all punctuation chars

pickle: read and write objects as binary file content
    dump(obj, file)
    load(file)

    file must be open with open() as usual
    depends on __getstate__ and __setstate__ members of object
        the built-in stuff has it already

copy
    deepcopy(obj)
        depends on __copy__ and __deepcopy__ existing on object (as built-ins have)

time
    deals with seconds (as floats)

    time() # since epoch
    perf_count() # higher-precision, less range
    sleep(time)
    localtime() # struct of time information fields

datetime
    time(hours, minutes)
    datetime.now()
    date.today()
    timedelta(days=1)
        total_seconds()
    datetime.now() + timedelta(days=1)

asyncio
    lets you do async/await type stuff
    async/await keywords are already in the language
        but you have to use asyncio to set up the top-level stuff to make it work

    async def f():
        return 10

    async def g():
        x = await f()
        return x

    val = asyncio.run(g)  # top-level, once per program, to kick off the async graph
    print(val)

    see also: async with, asyncio.gather(), asyncio.Lock (async/await friendly alternative to threading.Lock)

    lambdas do not support async (use nested functions instead)

argparse
    parser = argparse.ArgumentParser()
    parser.add_argument(...) to add each arg and info about it one by one
    args = parser.parse_args()
    args.field1, args.flag, etc. then available

dataclasses
    dataclass: decorator for defining a data class

logging
    basicConfig()
    logger = getLogger()
    logger.info('this is an info message')
    logger.error('this is an error!')

importlib
    reload(module): to reload a module (eg. to test changes)
        reloads all references to same module even if imported multiple times in different ways
    
typing
    Any
    TypeVar
    Union
    Optional

types: dynamic type creation stuff
    MappingProxyType: read-only view of a mapping with [] operator (such as dictionary)

enum
    Enum
    auto

json
    dump(mydict, myfile, indent=2)
        myfile is an open file (or sys.stdout for instance)
    mydict = load(myfile)

    s = dumps(mydict, indent=2)
    mydict = loads(s)

urllib: basic http requests and responses
    3rd party 'requests' library is considered better
http: status codes

threading
    multiple threads in python do not run concurrently because of Global Interpreter Lock (GIL)
        but can be preempted at any time to run a different one
        inc. in the middle of print statements
        good for IO-bound tasks, not for CPU-bound
    timeouts are in float seconds

    Thread
        thread = Thread(target=fn, args=(arg1,)) # can ommit args
        thread.start()
        thread.join(timeout) # optional, can wait forever too
        thread.is_alive()
            thread can die by normal termination or an exception

        the OOP version is to inherit from Thread and implement run() method

        no way to force kill threads
        main thread ending doesn't end other threads (even with sys.exit())

    Event
        e = Event()
        pass e to threads (eg. as arg) and also use it in parent
        e.set() to signal event
        e.wait() to wait for event (with optional timeout)
        set will wake up all waiting threads (not just 1 like a condition)

        this is the cleanest way to stop a running thread from another thread

    Lock
        l = Lock()
        pass l to threads (eg. as arg) and also use it in parent
        with l:
            # this is a critical section
        not reentrant!
        no special syntax keyword

    RLock
        exactly like Lock, but it's reentrant
    Semaphore
        exactly like Lock, but c'tor takes a count

    Condition
        acts as both a lock and an event
        but with wait(),notify(),notifyAll() like in Java
        you wait or notify within a critical section, unlike with events

    Timer
        thread = Timer(delay, fn)
        thread.start()

    Thread-local storage
        thread_local_storage = local()
            can call this globally in the main thread
            each thread will have its own copy
        thread can add and retrieve attributes from the thread_local_storage object
            eg. thread_local_storage.random = random.Random()
    
    ** no volatile keyword needed **
    ** no atomic types available **

multiprocessing
    uses parallel processes with IPC
        separate address spaces, copied from host process on creation (kind of)
        benefit = get around GIL and achieve true cpu-bound parallelism
        drawback = extra overhead and have to use IPC because no shared memory

    Process
        process = Process(target=fn, args=(arg1,))
        process.start()
        process.join()

    Pipe
        parent_conn, child_conn = Pipe()
            pass child_conn into child process via args
        child_conn.send()
        child_conn.close()
        parent_conn.recv()
        ** there is also a bi-directional version**
    Queue
        use same object in parent and child process
        put() and get()
    Value
        use same object in parent and child process
        'value' attribute (read and write) to share a data value
    Array
        use same object in parent and child process
        read/write via indexing/slicing to share data
    Manager
        for more complex objects and flexibility

concurrent.futures
    thread pools, executors, and futures all tied together

    ThreadPoolExecutor
        executor = ThreadPoolExecutor(max_workers=n)
            threads are created only as needed, so can set high
            set to 1 for single thread executor
        future = executor.submit(fn)

        future.done() boolean
        future.result() to block on and get value

        future.cancel() to request cancellation
            only works if didn't start yet

        executor.shutdown(wait=True)
            wait for tasks to stop
            doesn't forcefully stop them (no way to do that)

        futures executing in the thread pool can use the normal threading stuff
            locks, conditions, events, etc.

        asyncio.wrap_future(future) can be used to combine futures w/ asyncio async/await
            can await the result of this

    ProcessPoolExecutor
        works similarly to ThreadPoolExecutor
        but uses multiprocess instead of threads
            which means have to use IPC to communicate

[pip]
** for any interactive command, use -y or -n switch to make it non-interactive**

pip install termcolor
    install package and make importable
pip install --upgrade termcolor
    install or update package
pip install termcolor==2.3.0
    install specific version
    downgrades if necessary
pip install 'termcolor>=1.1,<1.2'
    install from a range of versions

pip show termcolor
    print info about installed package
pip uninstall termcolor
    uninstall a package

pip list
    show installed packages
pip list --outdated
    show list of outdated packages

wheel = pre-built binary package for python
    can be installed via pip
    see Modules notebook for more details about wheels when needed (esp. the Apple and TensorFlow stuff)

[conda]
conda update conda
conda env list
conda create -y -n conda-demo python=3.11
conda activate conda-demo
conda deactivate
conda install termcolor
conda env export >env.yaml
conda env remove -y -n conda-demo

conda manages:
    python version
    pip packages
conda does not manage:
    working directory
    apt packages
    node packages
conda manages things by messing with PATH as you activate and deactivate environments

[3rd party libraries]
termcolor
    from termcolor import colored
    print(colored('hi', 'red', attrs=['bold']) + ' there')
    colored() just makes text with escapes inside which can be concatted with un-colored text
sortedcontainers
    SortedList
    SortedKeyList
    SortedSet
    SortedDictionary
requests
    http request framework (more intuitive and fully-featured than urllib)
manim
    3blue1brown's animation library
    ** have notebook for it **
matplotlib
    graphing for jupyter notebooks (and other places)
    ** have notebook for it **
sympy
    symbolic computation library
    in many ways similar to tensorflow autodiff but more general
    ** have notebook for it **
scikit-learn
    some machine learning stuff
    I've used it for feature scaling for my TF models
    it can also do linear regression, PCA, etc.
    ** have notes in Google Drive for it **

[decorators]
function or callable class that wraps a function or class
    either returns it with some change, or returns another one wrapping it
eg. add attribute to function
eg. wrap a function in another function that does logging

def mydecorator(fn):
    def wrapper(*args, **kwargs):
        print('called')
        return fn(*args, **kwargs)
    return wrapper

@mydecorator # NOTE: @mymodule.mydecorator if in a module
def myfn(x):
    return x**2

myfn(10)
    returns 100 and also prints 'called' because of the decorator
    myfn gets replaced by the decorator version right away

built-in decorators:
    @property: lets you treat a method as an attribute
    @staticmethod
    @classmethod
    @abstractmethod

@mymodule.submodule.myclass.mydecorator
    decorator resolution follows normal rules
    the @ takes the end result of the resolution
    properties rely on this

[docstring]
""" multi-line strings in certain places
top of module for module documentation
top of function (below prototype - indented) for function docstring
top of class (below name - indented) for class docstring
after variable (eg. a constant)
can be one-line (all in one line) or have a space between the headline and the longer description

[type hints]
type hints aren't part of the actual execution of the program
    just for type checkers if you have one setup
    they can be arbitrarily left out, applied inconsistently, even wrong (if the type checker isn't aggressive or not set up)
the syntax is very similar to TypeScript

def add_numbers(a: int, b: int) -> int:
    return a + b

def print_this(a: int) -> None:
    print(a)

def do_stuff(a: int = 10, b: int = 5) -> int:
    return a + b

def add_numbers_list(l: list[int]) -> int:
    return sum(l)

def add_numbers_dictionary(d: dict[str, int]) -> int:
    return sum(d.values())

# This will still technically work if you pass <3 or >3 args.
def add_numbers_tuple(t: tuple[int, int, int]) -> int:
    return sum(t)

# aliases
Point = tuple[float, float, float]
Points = list[Point]

def centroid(p: Points) -> Point:
    return tuple(mean(axis) for axis in tuple(zip(*p)))

# variables
x: int = 5
y: Any = f() # any type (must be imported from Typing)

# collections.abc imports
# Callable (for functions)
def transform(list1: list[int], list2: list[int],
              fn: Callable[[int, int], int]) -> list[int]:
    return [fn(left, right) for left, right in zip(list1, list2)]
# Sequence (for sequences where you don't know if list, tuple, etc.)
def add_numbers(s: Sequence[int]) -> int:
    return sum(s)

# Generics (simulated)
from typing import TypeVar

T = TypeVar('T')

def first_thing(s: Sequence[T]) -> T:
    return s[0]

# Type union (shorthand for typing.Union[])
def make_str(a: int | str) -> str:
    if isinstance(a, int):
        return str(a)
    return a

# shorthand for typing.Optional[]
def coalesce(a: int | None) -> int:
    if a is None:
        return 0
    return a

[unit tests]
** see Unit Tests notebook **
not going to show much of it here
it works much like JUnit type stuff from other languages
class is instantiated to run test method
setup and teardown, etc.
mocking, faking, etc. available

[Jupyter]
throughout the notebooks, there are examples of jupyter usage
I did not include those here for now

[Performance]
variable name length can have a small impact on the size of the compiled bytecode
    but the difference is so small that readability is more important
variable name length can have a slight impact on compiling into bytecode
    but usually miniscule impact on the runtime performance
in practical applications, shorter variable names (when harming readability) is not recommended
    people do it in Leetcode because competition culture is different
    they are trying to write the code faster and shave off every nanosecond and byte
    they don't care about readability - just winning
don't worry too much about things like tuple packing/unpacking, etc.
    focus on algorithm order and readability, not individual clock cycles
    even on Leetcode those things don't have that much impact

[numpy]
import numpy as np
    not on GPU
        but still very efficient using CPU better than if you wrote pure Python code to do matrices
    top-level and instance functions to do same thing (in general)
    in general, assume mutations return a new copy unless in the "In-Place Mutation" section

    a lot of functions that operate on an array will accept a regular python list and convert for you
    a lot of functions that take a shape will accept a scalar value to mean 1D array of that size

a = np.array([[1, 2, 3], [4, 5, 6]]) # 2x3 matrix
    new independent object (not linked to original list passed in)
b = np.copy(a)
    independent copy
np.save(filename, a)
a = np.load(filename)
l = a.tolist()
    numpy array back to python list (of lists, etc.)

a.shape
    tuple (2, 3)
    a scalar has shape ()
a.reshape((2, 3))
    flattened order of elements will be same
    think of as nested loop with leftmost dimensions being outermost loop
    eg. 1, 2, 3, 4, 5, 6 in the above
a.reshape((2, -1))
    -1 means make this one whatever is needed to make the others work out
np.expand_dims(a, axis=2)
    new dimension of size 1 added (in this case after the columns)
    effectively just adding [] at some point in the hierarchy
    flattened element order is still the same and elements themselves are unchanged
a[..., np.newaxis]
    shorthand for the expand_dims call above
    the ... gets the full dimensions up until the last one (covered below)
    the np.newaxis tells np to add a new axis of size 1 here
np.swapaxes(A, 0, 1)
    transpose rows with columns (in this case)
    more general form of transposing a 2D matrix (arbitrary dimensions)
    actually changes element order in final result
len(a)
    rows (1st dimension size)

a.dtype
    eg. int, float, etc.
    can pass into np.array() as keyword arg to cast on creation
    numpy uses its own types in its arrays (eg. np.int64 when you say int)
a.as_type(float)
    copy with casting
    HINT: cast bool to int to get 0s and 1s
a.round()
    round floats to nearest (but stay floats)
    also np.round(anylist)

a[row][col] = scalar of shape ()
    type of the item is (in this case) np.int64
    it acts just like an int(with all operators) but will fail isinstance() test on int
        doing math between int and np.int64 will give you np.int64
    you can cast it with int() or use a[row][col].item() to get original type
    np.int64 and others act like matrices of shape ()
a[row] = a numpy array of shape (3,)
a[:,col] = a numpy array of shape (2,)
a[:-1,:-1] = a numpy array of shape (2,1)
a[..., col] = shorthand for putting : for all axes before the last one
    can use it for other axes, not just the first ones

np.zeros((3, 2))
np.zeros((3, 2), dtype=int)
np.zeros_like(a)
np.ones((3, 2))
np.ones_like(a)
np.full((3, 2), 100) # specific value
np.eye(3) # identity matrix 3x3

np.tril(a) # zeroes out upper right half (except the diagonal) to make lower triangular

np.arange(count)
    creates a 1D array from 0 to count - 1
    then you can reshape it into what shape you need
np.linespace(lower, inclusiveUpper, count)
    end up with floats
np.meshgrid(xs, ys)
    pairs up all combos of x and y coordinates from the two lists
    then gives back 2 matrices, one for x cordinates and one for y coordinates
    you can interpret matching elements from both as being a point in 2D space
    overall order is upper left corner -> upper right corner -> left on next row -> etc.

np.random.seed(42)
np.random.random_sample((3, 2))
    gets a 3x2 matrix of [0,1) floats
np.random.random((3, 2))
    same as random_sample
np.random.randn((3, 2))
    similar to random() but normal distrubition instead of uniform
    you can multiply and add to give it a mu and sigma

a == 2
    element-wise comparison
    will give a boolean matrix
a == b
    same but comparing elements of 2 arrays
a > 3
    similar idea to == but with < operator
    and others work too

-a
    element-wise negation
a**2
    element-wise power
a + 2
a * 2
a / 2
a - 2
    element-wise arithmetic
a + b
a / b
a * b
a - b
    element-wise arithmetic between two arrays
~a
    element-wise boolean inversion
    very useful for masking

a[a > 2]
    index masking
    a > 2 gives boolean matrix of same shape as a
    then a[mask] gives just the True elements
    result is reshaped to 1D
a[[False, True]]
    masking by row (because 1D mask)
    same overall structure as original but less rows
a[:, [False, True, True]]
    masking by column (because row is : slice)
    same overall structure as original but less columns
a[:, [1, 0, 1]]
    when masking with int instead of boolean, it is index rearranging instead
    the int tells what index from original you want there
    ok to include same one multiple times
HINT: both types of masking are useful for operating on multiple parallel matrices based on something like argmax

A @ B
A.dot(B)
np.dot(A, B)
np.matmul(A, B)
    these are all ways to multiply two compatible matrices (linear algebra style)
    if the matrices are 3D, then it is a parallel 2D multiplication across the 1st dimension
        if one is 3D, the other will be broadcasted to be multliplied in parallel with all 2D matrices
A.T
np.transpose(A)
    ways to transpose a matrix (linear algebra style)
np.diag(A)
    digonal vector of matrix
np.linalg.det(A)
    determinant of matrix
np.linalg.solve(A, b)
    get x to solve Ax = b
np.linalg.norm(A)
    L2 norm of matrix

np.dot(x, y)
    dot product of 2 1D vectors
np.linalg.norm(x)
    length of 1D vector
np.linalg.norm(x, axis=-1)
    length of vectors within matrix

np.sum(A)
    sum of all elements
    HINT: if boolean, it will be how many were True!
        eg. np.sum(a > 3)
np.mean(A)
    average over the elements of A
    HINT: if boolean, will give you ratio of elements that matched
        eg. np.mean(a > 3)
np.max(A), np.min(A)
    max and min across all elements
np.std(A)
    standard deviation
np.ptp(A)
    peak to peak
np.argmax(A), np.argmin(A)
    index of max or min element within array
    but it's the flattened index (reshaped to 1D)
The above aggregates can take an 'axis' keyword param
    then they aggreaget along the given axis (0 for 1st axis, etc.)
    -1 means last axis (-2, -3, etc. going to the left)
    along axis means collapse that axis and remove it from the shape

np.sign(A)
    -1, 0, or 1 element-wise
np.exp(A)
    element-wise e^x
np.log(A)
    element-wise ln
np.maximum(A, B)
    element-wise max across multiple matching arrays
    not the same as np.max()!
    very useful with broadcasting
        eg. np.maximum(A, 0) for ReLU element-wise
np.allclose(A, B)
    float near-equality element-wise
np.sin(A)
    element-wise trigonometric operation
np.clip(A, 2, 3)
    element-wise clip min 2 and max 3 (inclusive)
np.pi, np.e
    math constants

np.tile(A, 2)
    treat A as a tile and make 2 tiles along last axis (column)
np.tile(A, (2, 1))
    treat A as a tile and make 2 tiles along row axis and 1 along column
np.tile(A, (2, 2))
    make a square of 4 copies of A
np.concatenate([A, B], axis=1)
    treat A and B as separate tiles and put them together along the column axis
np.stack([A, B], axis=0)
    like concatenate but adds a new dimension to tile the matrices

Broadcasting
    when arrays mismatch in size in an operation that expects them to match
        key to vectorizing equations

    first, make the # of dimensions match
        turn scalars into [1, 1, 1, etc.]
        for others, add dimensions of size 1 to left of its dimensions
    from right to left in dimensions, skip equal ones and throw error if smaller one is not 1
        for each mismatched dimension, repeat smaller array along that axis to make match
        the one getting repeated can differ between individual dimensions

    eg. A + 5
        5 becomes [[5]] to have shape (1, 1)
        [[5]] repeated along column axis to become [[5, 5]]
        [[5, 5]] repeatd along row axis to become [[5, 5], [5, 5], [5, 5]]
        then add A + that to get the element-wise addition of 5 to A

    eg. A + b (matrix + 1D vector)
        if A has shape (3, 2) and b has shape (2,) for instance
        b becomes (1,2), then repeatd 3 times along row axies to make it (3, 2)
        so b is effectively a row vector added to all 3 rows
            its elements add to the corresponding columns, all the way down the rows
        NOTE: same result as if you gave a proper row vector (1,2) instead of (2,)

    eg. A + b (matrix + column vector by shape)
        b repeated twice along col axis
        so b is added to both columns (elements adding to rows)

    eg. row vector + column vector
        column vector has to be repeated along column axis
        row vector has to be repeated along row axis
        result is adding the tiled out result

    eg. 3d matrix times 2d matrix
        2d matrix has dimension 1 added to front to make it 3D
        then it's repeated along 1st dimension
        then the multiplication does a parallel multiplication along that dimension
        so the result is of multiplying each 2D matrix within the 3D one by the 2D matrix

Scalars
    s = np.array(10) # scalar value array
        shape is ()
    s.reshape(1)
        becomes shape (1,) which is 1D array with 1 value
    broadcasted as (1,1,1,1...) as described above
    s.item()
        get value from the scalar
        will fail if not a scalar
        back to original type (int instead of np.int64)
    s = a.squeeze()
        if all the dimensions of a are 1, this will give a scalar array for it

In-Place Mutation
    A[:, 1] = np.zeros(2)
        zero out column 1
    A[:, 1] = 0
        zero out column 1 by broadcasting
    A[1:, 1:] = 0
        zero out submatrix
        this is just example - you could send any array in (with or without broadcasting)
    np.fill_diagonal(A, 10)
        set all diagonal elements to 10
    A.fill(10)
        set all elements to 10
    A[A > 2] = 0
        zero out all elements greater than 2
    A[[0, 1]] = A[[1, 0]]
        swap rows 0 and 1
        the right side gives you just the rows you want in the order you want them
        then the left side assigns them into their final targets
        so it's not 100% in-place efficiency-wise, but the end result is that way

1D Vector Caveat
    some operations do the wrong thing if you have a 1D vector
    to be safe, properly reshape them to have a dimension of size 1 to make them 2D (or whatever)

Ragged Tensors
    structure looks right but some dimensions internally aren't right
    eg. 3 rows and 2 columns matrix, but maybe one of the rows is missing a column structurally
    this is actually allowed, but the results are unexpected, so don't ever let it happen

[pandas]
import pandas as pd

df = pd.DataFrame(dictionaryOfLists)
    keys = columns
    values = rows
    conceptually a table
df = pd.DataFrame(listOfDictionaries)
    same effect, but more duplication (eg. keys appearing in all the dicts)
    in this case, it's more like each dictionary is 1 row (1 datapoint)
df = pd.DataFrame(numpy_matrix, columns=listOfColumnNames)
df.info()
    technical info about the table
df.describe()
    statistics about the table (mean, max, stdev, etc.)
df.dtypes
    table of data types for columns

print(df)
    in jupyter, a formatted table will show up

df = pd.read_csv(filename)
    more formats, compression, etc.
    can pass index_col string to specify a column to treat as the index column
        replaces default index column you get
df.to_csv(filename)
    overwrites existing
    more options, formats, etc.

df[columnName] = a pd.Series object
df.columnName = same thing
a series is basically a named list of values
    you can make one like this: pd.Series([1, 2, 3], name="Age")
    can call methods like min(), max(), mean(), std() on it
df.max() and methods like that will work on all the columns
    give you that stat for each one
series.str.replace(name1, name2)
    returns a new Series with string replacement
df[columnName] = df[columnName].replace(str1, str2)
    can replace a series in table (in this case the one with replacements)

df.head(10)
    first 10 rows
df.tail(10)
    last 10 rows

df.shape
    like numpy

df[[name1, name2]]
    get a DataFrame consisting of a subset of columns
df[[name2, name1, name2]]
    supports reordering and repeats
df[name1] > 30
    series of booleans
df[name1]**2
    series with element-wise arithmetic
df[3:2]
    subset of rows
df[df["Age"] > 30]
    filtering rows
df.loc[df["Age"] > 35, ["Name", "Age"]]
    filter rows and cols at same time
df[3:2]["Age"]
    filtering rows and cols via operator
df.iterrows()
    to iterate rows

df["Name"] = "Bob"
    broadcasted to all rows
df["Name"] = series
    can replace a column, a row, etc. with transformed/broadcasted data
    can also add a new column this way
    spaces allowed in column names

df = df.rename(columns=dictionary)
df = df.drop(listOfColumns)
a = df.to_numpy()
    just the values - no names
a[["Name", "Age"]].values
    selective numpy array

dataframes and series can be plotted with their plot() methods
    can do various kinds of plots

df = df.groupby(columnName).mean()
    group by a column and take the mean of the rows for each group
df = df.groupby(columnName).count()
    group by a column and count the number of values of other columns
df = df.sort_values(by=columnName)
    sort by a column

pd.concat([df1, df2])

[TensorFlow]
** these aren't as complete as the notes I have in Google Drive - just some rough notebooks **

import tensorflow as tf
    will try to use GPU for tensors and CPU for datasets by default
        can be switched on the fly if GPU is configured (differs by platform)
    it has a lot of similarities to numpy
        eg. having its own data types to represent scalar numbers
        eg. has its own tensor operations that resemble numpy opeations with slightly different names
        assuming something will act like numpy is a good default
            eg. axes
            eg. slicing
            eg. broadcasting
            eg. scalars
            eg. shapes
    some differences from numpy
        a lot of functions/methods have different names but do the same thing
        a lot less methods on tf tensors (have to call functions instead)

with tf.device('/CPU:0'):
    # any tensor created here is on CPU
    # operations on that tensor are done on CPU
      # even after the context is over or if in GPU context
with tf.device('/GPU:0'):
    # any tensor created here is on GPU
    # operations on that tensor are done on GPU
      # even after the context is over or if in CPU context
What will move a tensor from CPU to GPU?
    copying it (tf.identity()) in GPU context
    mixed operations between CPU and GPU tensor (will make the result on GPU)
        original left where it is though
    keras model() call will create on GPU by default (unless in CPU context)

t = tf.constant([[1, 2, 3], [4, 5, 6]])
    create a tensor
    just like np.array()

t.device
    info about which CPU or GPU the tensor exists on

numpy functions that are the same here:
    stack, concat, transpose, concat, zeros, ones, reshape
numpy functions that are renamed here:
    np.copy() -> tf.identity()
    np.linalg.norm() -> tf.norm()
tf.numpy() to get a numpy array from a tensor
    for scalars, this has better behavior
        in that case, can effectively think of it as getting the value itself
t.assign_add(u) # += as in-place mutation

dataset = tf.data.Dataset.from_tensor_slices(t)
    create a dataset from a tensor with 1st dimension as the sample dimension
        each row is a sample
    fluent API for data processing pipeline
    you can also pass a tuple of tensors to have batches be tuples
        eg. x and y matrices for ground-truth labels
            see dataset.map() below for more details
    you can pass a list of tensors to have each tensor be a sample
for sample in dataset:
    print(sample)   # tensor with shape without 1st dimension
    NOTE: if batch size is not 1, then this will iterate over batches instead of samples
        from now on, we'll refer to iteration items as batches even if batch size is 1
    iterating a dataset DOES NOT CONSUME it like a generator
        you can iterate it multiple times and it will just start over
        in the model, each time iterating the whole thing is an EPOCH
tf.transpose(tf.concat(list(dataset), axis=0))
    reconstruct original tensor based on iteration
    simulates building a large tensor out multiple datasets, for instance
dataset = dataset.batch(2)
    change batch size from 1 (the default) to 2
    instead of individual samples, now you'll get a tensor of samples
    the structure will be like the original tensor, but the 1st dimension will be <= batch size
    eg. if have 3 samples, then iterate, you'll get:
        2 items
        1st item is a tensor with 2 rows (2 samples)
        2nd item is a tensor with 1 row (1 sample)
            still has outer dimension, unlike the batch size 1 case
    NOTE: model.fit() doesn't need you to respecify batch size if you set it here
    WARNING: trying to override batch size again will throw an exception
tensor = dataset.reduce(seed, fn)
    reduces the dataset down to a tensor
    fn will take (accumulated, nextValue)
    eg. to count batches in a dataset (which will consume it):
        dataset.reduce(0, lambda x,_: x + 1).numpy()
dataset = dataset.map(fn)
    run a function to map batch to batch for each batch in the pipeline
    eg. dataset = dataset.map(lambda batch: (batch[:, :-1], batch[:, -1]))
        makes each batch into a tuple
        item 0 = the batch with the last column missing
        item 1 = the last column values for the batch
        model.fit() expects dataset batches to look like this
            item1 of batch tuple = ground truth labels
            can leave off y arg of model.fit()
    WARNING: if batches are tuples with ground-truth labels
        map() will automatically unwrap to 2 args for the lambda instead of 1 tuple
        but you still must return a tuple to keep the structure
        a benefit of this is you can write a function with the 2nd arg defaulted to None
            then it will work with or without ground-truth labels
    NOTE: from_tensor_slices can also take a tuple of tensors to avoid having to do this map
dataset = dataset.shuffle(buffer_size=3, seed=42)
    make the dataset return samples in shuffled order
    shuffing is both within and between batches (ignores batches)
    buffer_size is typically the whole # of samples as it affects the granularity of Shuffling
    shuffling happens again between each full iteration (epoch)
dataset = dataset.take(n)
    truncate the dataset to the first n batches
    be careful where you place it relative to shuffle
dataset = dataset.skip(n)
    skip the first n batches
    same considerations as for take()
train, validation = dataset.take(2), dataset.skip(2)
    for the split to be consistent and non-overlapping,
        make sure to shuffle AFTER this and not before (w/ same seed in both places)
next(iter(dataset.take(1)))
    getting the 1st batch
dataset = dataset.prefetch(tf.data.AUTOTUNE)
    prefetch new batches into memory while model is working on the oldest batches
    this keeps the CPU and disk busy while the GPU is training, validating, etc.
    AUTOTUNE lets the system figure out the # of batches
    this mostly applies to datasets loaded form disk, such as CSV
    this typically goes at the END of the pipeline
dataset = tf.data.DataSet.zip(dictionaryOfDataset)
    makes a dictionary-like dataset
    keys are maintained in batches emitted by the dataset
    batches are emitted from each dataset as values
    each "batch" is a dictionary of batches basically
    this is the format used by CSV datasets as well
    make batch sizes of the individual datasets consistent or this won't work correctly
    you can also put the dictinoary in a tuple with a tensor (eg. ground truth labels)
    map() can be used to translate between dictionary and tensor datasets manually later
dataset = tf.data.DataSet.from_generator(fn, output_signature=...)
    each value yielded by the generator is a sample
dataset = tf.data.experimental.make_csv_dataset('titanic.csv', batch_size=1)
    streamed from disk and prefetched with AUTOTUNE by default
    1st line = column names by default
    shuffled on each epoch by default
    can provide label_name to extract a ground truth label (and make tuple dataset automatically)
    gives a dictionary dataset with column names as keys
    if you don't specify num_epochs, it will repeat the dataset forever without reiterating
    don't call .batch() on a csv dataset - use the param in make_csv_dataset()

Keras
    model = tf.keras.models.Sequential([tf.keras.layers.Dense(1, activation='sigmoid')])
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),
              loss='binary_crossentropy',
              metrics=['accuracy'])
    history = model.fit(
        train_dataset,
        validation_data=validation_dataset,
        epochs=epochs
    ) # use history.histor['accuracy'] for plotting
    model.predict(dataset)
        returns numpy array instead of tf tensor!
            also doesn't batch the output at all (no dimension for batch, just samples)
        if ground truth labels (tuple dataset), will just ignore the labels
        if you correlate results back to inputs, be careful of shuffling
            don't need to shuffle in prediction - that is for training
    model.predict_on_batch(batch)
        you can batch the dataset yourself and predict 1 batch at a time
        eg. if you want to write out to disk in batches

    tf.keras.layers.Input(shape=shape)
        input layer to put first in your sequential model
        in cases where tensor shape is ambiguous, this helps lock it down
        the shape should not contain the batch dimension (that will be added by the model)
    
    Functional API (alternative to above sequential api)
        use the same layers but patch together your own graph
            good for more complicated than simple sequential networks
                eg. node needs to go two places

        input = tf.keras.layers.Input(shape=(5, 5))
        output = tf.keras.layers.Dense(1)(input)
            layer created and then called as a function on the input
            you chain together a graph this way

        model = tf.keras.Model(inputs=[input], outputs=[output])
            list all inputs and outputs so keras knows the graph
            the object passed to inputs is somewhat flexible
            can be a list of tensors, dictionary of tensors, etc.
                batch dimension smartly added to tensors inside with None value

        mode.input_shape has the input shape with batch dimension added
            uses None as the size of the dimension to show that it's flexible
        you can use something like a tf.keras.layers.Concatenate to transform the input
        you can use the inputs inside the structure of the input to the model directly in your graph
            then the model will expect that outer structure, but the graph will know how to patch in the proper nodes

        you can do tensor operations in between
        the things that you're passing between layers are actually tensors in GRAPH MODE
            with shape having None as 1st dimension for batches (within the inner structure as appropriate)
            can do tensor operations on a graph mode tensor, and it will give another graph mode tensor
                shape will follow naturally and may still contain None dimensions
        once you create a layer, it deals with batched shapes all the way through everywhere
        NOTE: you are allowed to pass None for unknown dimensions in your shapes too (not just for batching)

        if you call a layer on an eager mode tensor (such as tf.constant() result)
            it will evaluate in eager mode

        @tf.function decorator can be applied on a function to compile it to graph mode
            then it can be used in a training loop in graph mode to run more efficiently
            if only run once, no difference - because 1st run is when it gets compiled/traced
            graph mode is more efficient than eager mode, so it is used for training loops in production
    
AutoDiff
    this is the low-level old-fashioned primitive way to train models
        keras uses it internally without exposing it to you
        if you use this approach, you would do your own math and apply your own gradients
        this is how it worked before keras came along

    x = tf.Variable(10.0, name='x', dtype=tf.float32)
        x is a variable assigned 10
        autodiff wants floats instead of integers
    with tf.GradientTape() as tape:
        # in context of graident tape, operations are tracked
        y = x**2
            # y is a tensor (potentially in graph mode, potentially in eager mode)
            # any operation on a variable gives you a tensor (for tracking in autodiff)
        z = y + 1
            # z is another tensor
            # operations on tensors return other tensors and track for autodiff
    dz_dx = tape.gradient(z, x)
        get gradient of a variable with respect to the input variable
        this will also be a tensor
        you can then use this tensor to update weights, etc.
    
        second param could be a list instead if have multiple input variables to differentiate against
            then you get back a tuple

        this works transparently for matrices
            input variables can be matrices, operations can be matrix operations including @
            gradients can be matrices
            shape of gradient with respect to matrix will match the matrix
    Gradient Descent Example
        make A and x tf.Variable objects
        calculate b = A@x and J = tf.norm(b) within tape context
        get gradient of J with respect to A
        subtract learning rate times gradient from A

[Web Backend]
** see Web Backend notebook to study how to make backends **

here is a brief summary of topics to keep in mind:
    Flask - for HTTP REST API from Python (server-side)
    SQLAlchemy - for database support
    gunicorn - to host a Flask app in production (inc. in docker, kubernetes, etc)
    gRPC - for service-to-service communication
    requests - HTTP requests from service-to-service
    
[Interview Tips]
    as usual, be able to do your own linked list nodes, binary search, etc.
    negative indexing, though not changing O(), is slower in leetcode problems
    can use a dict like you'd use an anonymous object in JS
        inc. literal
        unless need < operator, then used named tuple
    prefer comprehensions to map(), filter(), etc. as it's more idiomatic nowadays
    reverse() and reversed() don't take index
        idiomatic way is to use slicing
        same goes with sort
    idiomatic to declare variables on one line using tuple unpacking